{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLiIuRRfG_jX"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from random import randint, choice, random, seed, sample, shuffle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Set random seed for reproducibili\n",
        "seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Number of propositional variables\n",
        "t_nu = 5  # Variables: p, q, r, s, t\n",
        "\n",
        "# Define numerical values for symbols\n",
        "symb = {\n",
        "    \"DE\": t_nu + 1,  # ⊢ DERIVES (not used in formula generation)\n",
        "    \"LB\": t_nu + 2,  # (\n",
        "    \"RB\": t_nu + 3,  # )\n",
        "    \"NO\": t_nu + 4,  # ¬\n",
        "    \"TH\": t_nu + 5,  # →\n",
        "    \"OR\": t_nu + 6,  # ∨\n",
        "    \"AN\": t_nu + 7,  # ∧\n",
        "    \"FA\": t_nu + 8   # ⊥\n",
        "}\n",
        "\n",
        "# Reverse mapping from numerical values to symbols\n",
        "symb_reverse = {0: \"\"}\n",
        "for i in range(1, t_nu + 1):\n",
        "    symb_reverse[i] = chr(ord('p') + i - 1)\n",
        "symb_reverse.update({\n",
        "    t_nu + 1: \"⊢\",\n",
        "    t_nu + 2: \"(\",\n",
        "    t_nu + 3: \")\",\n",
        "    t_nu + 4: \"¬\",\n",
        "    t_nu + 5: \"→\",\n",
        "    t_nu + 6: \"∨\",\n",
        "    t_nu + 7: \"∧\",\n",
        "    t_nu + 8: \"⊥\"\n",
        "})\n",
        "\n",
        "# Mapping from symbols to numerical values\n",
        "symb_map = {v: k for k, v in symb_reverse.items()}\n",
        "\n",
        "# Function to generate a random propositional variable\n",
        "def rd_f(t_nu):\n",
        "    return randint(1, t_nu)\n",
        "\n",
        "# Function to recursively generate well-formed formulas (WFFs)\n",
        "def gen_wff(form, depth, max_depth=3):\n",
        "    if depth >= max_depth or random() < 0.6:\n",
        "        return form\n",
        "    else:\n",
        "        rule = choice(wff_rules)\n",
        "        subform = rule(form, depth)\n",
        "        return subform\n",
        "\n",
        "# Rules for generating well-formed formulas\n",
        "def cona(form1, depth):  # Conjunction A\n",
        "    return [form1, symb[\"AN\"], gen_wff(rd_f(t_nu), depth + 1)]\n",
        "\n",
        "def conb(form1, depth):  # Conjunction B\n",
        "    return [gen_wff(rd_f(t_nu), depth + 1), symb[\"AN\"], form1]\n",
        "\n",
        "def disa(form1, depth):  # Disjunction A\n",
        "    return [form1, symb[\"OR\"], gen_wff(rd_f(t_nu), depth + 1)]\n",
        "\n",
        "def disb(form1, depth):  # Disjunction B\n",
        "    return [gen_wff(rd_f(t_nu), depth + 1), symb[\"OR\"], form1]\n",
        "\n",
        "def th_a(form1, depth):  # Implication A\n",
        "    return [form1, symb[\"TH\"], gen_wff(rd_f(t_nu), depth + 1)]\n",
        "\n",
        "def th_b(form1, depth):  # Implication B\n",
        "    return [gen_wff(rd_f(t_nu), depth + 1), symb[\"TH\"], form1]\n",
        "\n",
        "def neg(form1, depth):  # Negation\n",
        "    return [symb[\"NO\"], form1]\n",
        "\n",
        "wff_rules = [cona, conb, disa, disb, th_a, th_b, neg]\n",
        "\n",
        "# Function to convert a formula to a string\n",
        "def formula_to_string(f):\n",
        "    if isinstance(f, int):\n",
        "        return symb_reverse[f]\n",
        "    elif isinstance(f, list):\n",
        "        if len(f) == 2 and f[0] == symb[\"NO\"]:  # Negation\n",
        "            return symb_reverse[symb[\"NO\"]] + formula_to_string(f[1])\n",
        "        elif len(f) == 3:\n",
        "            left = formula_to_string(f[0])\n",
        "            op = symb_reverse[f[1]]\n",
        "            right = formula_to_string(f[2])\n",
        "            return f'({left} {op} {right})'\n",
        "        else:\n",
        "            return ''.join(formula_to_string(subf) for subf in f)\n",
        "    else:\n",
        "        return str(f)\n",
        "\n",
        "# Derivation rules for IPL (Intuitionistic Propositional Logic)\n",
        "def fa_e(prem):  # Falsum Elimination: From ⊥, derive any formula\n",
        "    return rd_f(t_nu)\n",
        "\n",
        "def no_e(prem):  # Negation Elimination: From ¬A and A, derive ⊥\n",
        "    return symb[\"FA\"]\n",
        "\n",
        "def n_ia(premises):  # NEGATION INTRODUCTION\n",
        "    # From A ⊢ ⊥ infer ¬A\n",
        "    return [symb[\"NO\"], premises[0]]\n",
        "\n",
        "def an_i(prem):  # Conjunction Introduction: From A and B, derive A ∧ B\n",
        "    return [prem[0], symb[\"AN\"], prem[1]]\n",
        "\n",
        "def a_ea(prem):  # Conjunction Elimination A: From A ∧ B, derive A\n",
        "    return prem[0][0]\n",
        "\n",
        "def a_eb(prem):  # Conjunction Elimination B: From A ∧ B, derive B\n",
        "    return prem[0][2]\n",
        "\n",
        "def t_ea(prem):  # Implication Elimination A (Modus Ponens): From A and A → B, derive B\n",
        "    return prem[1][2]\n",
        "\n",
        "def t_eb(prem):  # Implication Elimination B: From A → B and A, derive B\n",
        "    return prem[0][2]\n",
        "\n",
        "def th_i(prem):  # Implication Introduction: From assumption A to derive B, infer A → B\n",
        "    return [prem[0], symb[\"TH\"], prem[1]]\n",
        "\n",
        "def o_ia(prem):  # Disjunction Introduction A: From A, derive A ∨ B\n",
        "    return [prem[0], symb[\"OR\"], gen_wff(rd_f(t_nu), 0)]\n",
        "\n",
        "def o_ib(prem):  # Disjunction Introduction B: From A, derive B ∨ A\n",
        "    return [gen_wff(rd_f(t_nu), 0), symb[\"OR\"], prem[0]]\n",
        "\n",
        "# Additional rule for Classical Propositional Logic (CPL)\n",
        "def d_ne(prem):  # Double Negation Elimination: From ¬¬A, derive A\n",
        "    return prem[0][1]\n",
        "\n",
        "# List of rules for IPL and CPL\n",
        "ipl_rules = [fa_e, no_e, n_ia, an_i, a_ea, a_eb, t_ea, t_eb, th_i, o_ia, o_ib]\n",
        "cpl_rules = ipl_rules + [d_ne]\n",
        "\n",
        "# Function to check applicability of a rule to premises\n",
        "def check(rule, prem):\n",
        "    if len(prem) == 1:\n",
        "        if rule == fa_e:  # Falsum Elimination\n",
        "            if prem[0] == symb[\"FA\"]:\n",
        "                return True\n",
        "        if rule in [a_ea, a_eb]:  # Conjunction Elimination\n",
        "            if isinstance(prem[0], list) and len(prem[0]) == 3 and prem[0][1] == symb[\"AN\"]:\n",
        "                return True\n",
        "        if rule in [o_ia, o_ib]:  # Disjunction Introduction\n",
        "            return True\n",
        "        if rule == d_ne:  # Double Negation Elimination\n",
        "            if isinstance(prem[0], list) and len(prem[0]) == 2 and prem[0][0] == symb[\"NO\"]:\n",
        "                subf = prem[0][1]\n",
        "                if isinstance(subf, list) and len(subf) == 2 and subf[0] == symb[\"NO\"]:\n",
        "                    return True\n",
        "    elif len(prem) == 2:\n",
        "        if symb[\"FA\"] in prem:\n",
        "            if rule in [n_ia]:  # Negation Introduction\n",
        "                return True\n",
        "        else:\n",
        "            if rule == no_e:  # Negation Elimination\n",
        "                if (is_negation(prem[0]) and prem[0][1] == prem[1]) or \\\n",
        "                   (is_negation(prem[1]) and prem[1][1] == prem[0]):\n",
        "                    return True\n",
        "            if rule == an_i:  # Conjunction Introduction\n",
        "                return True\n",
        "            if rule == t_ea:  # Implication Elimination A\n",
        "                if is_implication(prem[1]) and prem[1][0] == prem[0]:\n",
        "                    return True\n",
        "            if rule == t_eb:  # Implication Elimination B\n",
        "                if is_implication(prem[0]) and prem[0][0] == prem[1]:\n",
        "                    return True\n",
        "            if rule == th_i:  # Implication Introduction\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# Helper functions\n",
        "def is_negation(f):\n",
        "    return isinstance(f, list) and len(f) == 2 and f[0] == symb[\"NO\"]\n",
        "\n",
        "def is_implication(f):\n",
        "    return isinstance(f, list) and len(f) == 3 and f[1] == symb[\"TH\"]\n",
        "\n",
        "# Function to get applicable rules for given premises\n",
        "def get_applicable_rules(premises, rules):\n",
        "    applicable_rules = []\n",
        "    for rule in rules:\n",
        "        if check(rule, premises):\n",
        "            applicable_rules.append(rule)\n",
        "    return applicable_rules\n",
        "\n",
        "# Function to generate a derivation\n",
        "def generate_derivation(rules, max_steps=2):\n",
        "    premises = [gen_wff(rd_f(t_nu), 0), gen_wff(rd_f(t_nu), 0)]\n",
        "    formulas = premises.copy()\n",
        "    num_steps = randint(2, max_steps)\n",
        "    for _ in range(num_steps):\n",
        "        # Choose 1 or 2 formulas from previous formulas\n",
        "        num_premises = choice([1, 2])\n",
        "        if len(formulas) < num_premises:\n",
        "            num_premises = len(formulas)\n",
        "        selected_premises = sample(formulas, num_premises)\n",
        "        # Get applicable rules\n",
        "        applicable_rules = get_applicable_rules(selected_premises, rules)\n",
        "        if applicable_rules:\n",
        "            rule = choice(applicable_rules)\n",
        "            # Apply the rule\n",
        "            new_formula = rule(selected_premises)\n",
        "            formulas.append(new_formula)\n",
        "        else:\n",
        "            # No applicable rules, stop derivation\n",
        "            break\n",
        "    return [premises, t_nu + 1, formulas[-1]]\n",
        "\n",
        "# Function to flatten formulas into tokens for model input\n",
        "def flatten_formula(f):\n",
        "    if isinstance(f, int):\n",
        "        return [f]\n",
        "    elif isinstance(f, list):\n",
        "        tokens = []\n",
        "        if len(f) == 2 and f[0] == symb[\"NO\"]:  # Negation\n",
        "            tokens.append(f[0])\n",
        "            tokens.extend(flatten_formula(f[1]))\n",
        "        elif len(f) == 3:\n",
        "            tokens.append(symb[\"LB\"])\n",
        "            tokens.extend(flatten_formula(f[0]))\n",
        "            tokens.append(f[1])\n",
        "            tokens.extend(flatten_formula(f[2]))\n",
        "            tokens.append(symb[\"RB\"])\n",
        "        else:\n",
        "            for subf in f:\n",
        "                tokens.extend(flatten_formula(subf))\n",
        "        return tokens\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "# Prepare data for the transformer model\n",
        "def prepare_data(num_samples=1000):\n",
        "    data = []\n",
        "    for _ in range(num_samples):\n",
        "        derivation = generate_derivation(ipl_rules)\n",
        "        if derivation:\n",
        "            flattened_derivation = flatten_formula(derivation)\n",
        "            data.append(flattened_derivation)  # Remove last separator\n",
        "    # Pad sequences to the same length\n",
        "    max_len = max(len(seq) for seq in data)\n",
        "    padded_data = []\n",
        "    for seq in data:\n",
        "        padded_seq = seq + [0] * (max_len - len(seq))  # Pad with 0s\n",
        "        padded_data.append(padded_seq)\n",
        "    return padded_data\n",
        "\n",
        "# Transformer Model Components\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, num_heads, hidden_dim, num_layers, dropout, max_seq_len=500):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_len, emb_dim))\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src) + self.positional_encoding[:, :src.size(1), :].to(src.device)\n",
        "        embedded = embedded.permute(1, 0, 2)  # [seq_len, batch_size, emb_dim]\n",
        "        output = self.transformer_encoder(embedded)\n",
        "        return output\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, num_heads, hidden_dim, num_layers, dropout, max_seq_len=500):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_len, emb_dim))\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
        "\n",
        "    def forward(self, tgt, memory):\n",
        "        embedded = self.embedding(tgt) + self.positional_encoding[:, :tgt.size(1), :].to(tgt.device)\n",
        "        embedded = embedded.permute(1, 0, 2)  # [seq_len, batch_size, emb_dim]\n",
        "        output = self.transformer_decoder(embedded, memory)\n",
        "        output = output.permute(1, 0, 2)  # [batch_size, seq_len, emb_dim]\n",
        "        prediction = self.fc_out(output)\n",
        "        return prediction\n",
        "\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg_len):\n",
        "        memory = self.encoder(src)\n",
        "        # Start token assumed to be index 0\n",
        "        tgt = torch.zeros((src.size(0), trg_len), device=self.device, dtype=torch.long)\n",
        "        output = self.decoder(tgt, memory)\n",
        "        return output\n",
        "\n",
        "# Training the model\n",
        "def train_model(model, data_loader, num_epochs=10, batch_size=32, learning_rate=1e-4):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    model.train()\n",
        "    device = model.device\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch in data_loader:\n",
        "            src = torch.tensor(batch, dtype=torch.long).to(device)\n",
        "            max_len = len(src[0])\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src, max_len)\n",
        "            # Compute loss based on syntactic distance\n",
        "            loss = loss_function(output)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n",
        "\n",
        "# Putting it all together\n",
        "def main():\n",
        "    # Prepare data\n",
        "    data = prepare_data(num_samples=1000)\n",
        "    vocab_size = t_nu + 9  # Number of symbols\n",
        "\n",
        "    # Model parameters\n",
        "    input_dim = vocab_size\n",
        "    output_dim = vocab_size\n",
        "    emb_dim = 256\n",
        "    num_heads = 8\n",
        "    hidden_dim = 512\n",
        "    num_layers = 3\n",
        "    dropout = 0.1\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Initialize the model\n",
        "    encoder = TransformerEncoder(input_dim, emb_dim, num_heads, hidden_dim, num_layers, dropout)\n",
        "    decoder = TransformerDecoder(output_dim, emb_dim, num_heads, hidden_dim, num_layers, dropout)\n",
        "    model = Seq2SeqTransformer(encoder, decoder, device).to(device)\n",
        "\n",
        "    batch_size = 32\n",
        "    data_loader = DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x) # Custom collate_fn\n",
        "\n",
        "    # Train the model\n",
        "    train_model(model, data_loader, num_epochs=10, batch_size=32, learning_rate=1e-4)\n",
        "\n",
        "    # Testing the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_premises, test_derivation_steps = generate_derivation(ipl_rules)\n",
        "        if test_derivation_steps:\n",
        "            input_formulas = test_premises + [test_derivation_steps[-1]['conclusion']]\n",
        "            input_tokens = []\n",
        "            for f in input_formulas:\n",
        "                input_tokens.extend(flatten_formula(f))\n",
        "                input_tokens.append(symb[\"DE\"])\n",
        "            src = torch.tensor([input_tokens[:-1]], dtype=torch.long).to(device)\n",
        "            trg_len = 50  # Maximum length of generated sequence\n",
        "            output = model(src, trg_len)\n",
        "            predicted_tokens = output.argmax(dim=-1).squeeze(0).tolist()\n",
        "            # Convert tokens to formulas\n",
        "            predicted_formulas = []\n",
        "            current_formula = []\n",
        "            for tok in predicted_tokens:\n",
        "                if tok == symb[\"DE\"]:\n",
        "                    if current_formula:\n",
        "                        predicted_formulas.append(current_formula)\n",
        "                        current_formula = []\n",
        "                else:\n",
        "                    current_formula.append(tok)\n",
        "            if current_formula:\n",
        "                predicted_formulas.append(current_formula)\n",
        "            # Print the predicted derivation\n",
        "            print(\"\\nPredicted Derivation:\")\n",
        "            for f_tokens in predicted_formulas:\n",
        "                f = reconstruct_formula(f_tokens)\n",
        "                print(formula_to_string(f))\n",
        "\n",
        "# Function to reconstruct formula from tokens\n",
        "def reconstruct_formula(tokens):\n",
        "    stack = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        tok = tokens[i]\n",
        "        if tok == symb[\"LB\"]:\n",
        "            stack.append(symb[\"LB\"])\n",
        "            i += 1\n",
        "        elif tok == symb[\"RB\"]:\n",
        "            # Pop elements until '('\n",
        "            sub_formula = []\n",
        "            while stack and stack[-1] != symb[\"LB\"]:\n",
        "                sub_formula.insert(0, stack.pop())\n",
        "            if stack and stack[-1] == symb[\"LB\"]:\n",
        "                stack.pop()  # Remove '('\n",
        "            stack.append(sub_formula)\n",
        "            i += 1\n",
        "        else:\n",
        "            stack.append(tok)\n",
        "            i += 1\n",
        "    # The final formula is on the stack\n",
        "    if stack:\n",
        "        return stack[0]\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Ni5F4D5tvxm5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "351884d7-a3c8-4e2b-fe0a-4d12801815aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 10, 2], 2, 2, 2, 2, [2, 10, 2], 2, 2, 2, 2]\n",
            "[[2, 10, 2], 2, 2, 2, 2, 2, 2, 2, 2, [2, 10, 2]]\n",
            "[2, [2, 10, 2], 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "[[None, 10, 2], 2, 2, [2, 10, [2, 10, 2]], 2, 2, 2, 2, 2, [2, 10, [2, 10, 2]]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-972361fa1b80>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-53-0b22e4ae1d0b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# Testing the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-0b22e4ae1d0b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, data_loader, num_epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;31m# Compute loss based on syntactic distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msyntactic_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-e430be182745>\u001b[0m in \u001b[0;36msyntactic_loss\u001b[0;34m(predicted)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msyntactic_error_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# Now check the logical derivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mlogical_error_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_derivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_formulas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0;31m# Accumulate logical loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlogical_error_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-e430be182745>\u001b[0m in \u001b[0;36mcheck_derivation\u001b[0;34m(parsed_formulas)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpremises\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknown_formulas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_prem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                 \u001b[0;31m# Get applicable rules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0mapplicable_rules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_applicable_rules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpremises\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mipl_rules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mrule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mapplicable_rules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpremises\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-0b22e4ae1d0b>\u001b[0m in \u001b[0;36mget_applicable_rules\u001b[0;34m(premises, rules)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0mapplicable_rules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpremises\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0mapplicable_rules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mapplicable_rules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-0b22e4ae1d0b>\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(rule, prem)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;31m# Function to check applicability of a rule to premises\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrule\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfa_e\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Falsum Elimination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teacher"
      ],
      "metadata": {
        "id": "DRt3qdI5SGyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "4lMHpkfKURGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from random import randint, choice, random, seed, sample, shuffle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Number of propositional variables\n",
        "t_nu = 5  # Variables: p, q, r, s, t\n",
        "\n",
        "# Define numerical values for symbols\n",
        "symb = {\n",
        "    \"DE\": t_nu + 1,  # ⊢ DERIVES (not used in formula generation)\n",
        "    \"LB\": t_nu + 2,  # (\n",
        "    \"RB\": t_nu + 3,  # )\n",
        "    \"NO\": t_nu + 4,  # ¬\n",
        "    \"TH\": t_nu + 5,  # →\n",
        "    \"OR\": t_nu + 6,  # ∨\n",
        "    \"AN\": t_nu + 7,  # ∧\n",
        "    \"FA\": t_nu + 8,  # ⊥\n",
        "    \"PAD\": 0,        # Padding token\n",
        "    \"SOS\": t_nu + 9, # Start of sequence\n",
        "    \"EOS\": t_nu + 10 # End of sequence\n",
        "}\n",
        "\n",
        "# Reverse mapping from numerical values to symbols\n",
        "symb_reverse = {0: \"\"}\n",
        "for i in range(1, t_nu + 1):\n",
        "    symb_reverse[i] = chr(ord('p') + i - 1)\n",
        "symb_reverse.update({\n",
        "    t_nu + 1: \"⊢\",\n",
        "    t_nu + 2: \"(\",\n",
        "    t_nu + 3: \")\",\n",
        "    t_nu + 4: \"¬\",\n",
        "    t_nu + 5: \"→\",\n",
        "    t_nu + 6: \"∨\",\n",
        "    t_nu + 7: \"∧\",\n",
        "    t_nu + 8: \"⊥\",\n",
        "    t_nu + 9: \"<SOS>\",\n",
        "    t_nu + 10: \"<EOS>\"\n",
        "})\n",
        "\n",
        "# Mapping from symbols to numerical values\n",
        "symb_map = {v: k for k, v in symb_reverse.items()}\n",
        "\n",
        "# Function to generate a random propositional variable\n",
        "def rd_f(t_nu):\n",
        "    return randint(1, t_nu)\n",
        "\n",
        "# Function to recursively generate well-formed formulas (WFFs)\n",
        "def gen_wff(form=None, depth=0, max_depth=3):\n",
        "    if depth >= max_depth or random() < 0.6:\n",
        "        if form is None:\n",
        "            return rd_f(t_nu)\n",
        "        else:\n",
        "            return form\n",
        "    else:\n",
        "        if form is None:\n",
        "            form = gen_wff(depth=depth + 1, max_depth=max_depth)\n",
        "        rule = choice(wff_rules)\n",
        "        subform = rule(form, depth)\n",
        "        return subform\n",
        "\n",
        "# Rules for generating well-formed formulas\n",
        "def cona(form1, depth):  # Conjunction A\n",
        "    return [form1, symb[\"AN\"], gen_wff(None, depth + 1)]\n",
        "def conb(form1, depth):  # Conjunction B\n",
        "    return [gen_wff(None, depth + 1), symb[\"AN\"], form1]\n",
        "def disa(form1, depth):  # Disjunction A\n",
        "    return [form1, symb[\"OR\"], gen_wff(None, depth + 1)]\n",
        "def disb(form1, depth):  # Disjunction B\n",
        "    return [gen_wff(None, depth + 1), symb[\"OR\"], form1]\n",
        "def th_a(form1, depth):  # Implication A\n",
        "    return [form1, symb[\"TH\"], gen_wff(None, depth + 1)]\n",
        "def th_b(form1, depth):  # Implication B\n",
        "    return [gen_wff(None, depth + 1), symb[\"TH\"], form1]\n",
        "def neg(form1, depth):    # Negation\n",
        "    return [symb[\"NO\"], form1]\n",
        "\n",
        "wff_rules = [cona, conb, disa, disb, th_a, th_b, neg]\n",
        "\n",
        "# Function to convert a formula to a string\n",
        "def formula_to_string(f):\n",
        "    if isinstance(f, int):\n",
        "        return symb_reverse[f]\n",
        "    elif isinstance(f, list):\n",
        "        if len(f) == 2 and f[0] == symb[\"NO\"]:  # Negation\n",
        "            return symb_reverse[symb[\"NO\"]] + formula_to_string(f[1])\n",
        "        elif len(f) == 3:\n",
        "            left = formula_to_string(f[0])\n",
        "            op = symb_reverse[f[1]]\n",
        "            right = formula_to_string(f[2])\n",
        "            return f'({left} {op} {right})'\n",
        "        else:\n",
        "            return ''.join(formula_to_string(subf) for subf in f)\n",
        "    else:\n",
        "        return str(f)\n",
        "\n",
        "# Derivation rules for IPL (Intuitionistic Propositional Logic)\n",
        "def fa_e(prem):  # Falsum Elimination: From ⊥, derive any formula\n",
        "    return gen_wff()\n",
        "def no_e(prem):  # Negation Elimination: From ¬A and A, derive ⊥\n",
        "    return symb[\"FA\"]\n",
        "def n_ia(premises):  # Negation Introduction\n",
        "    # From A ⊢ ⊥ infer ¬A\n",
        "    return [symb[\"NO\"], premises[0]]\n",
        "def an_i(prem):  # Conjunction Introduction: From A and B, derive A ∧ B\n",
        "    return [prem[0], symb[\"AN\"], prem[1]]\n",
        "def a_ea(prem):  # Conjunction Elimination A: From A ∧ B, derive A\n",
        "    return prem[0][0]\n",
        "def a_eb(prem):  # Conjunction Elimination B: From A ∧ B, derive B\n",
        "    return prem[0][2]\n",
        "def t_ea(prem):  # Implication Elimination A (Modus Ponens): From A and A → B, derive B\n",
        "    return prem[1][2]\n",
        "def t_eb(prem):  # Implication Elimination B: From A → B and A, derive B\n",
        "    return prem[0][2]\n",
        "def th_i(prem):  # Implication Introduction: From assumption A to derive B, infer A → B\n",
        "    return [prem[0], symb[\"TH\"], prem[1]]\n",
        "def o_ia(prem):  # Disjunction Introduction A: From A, derive A ∨ B\n",
        "    return [prem[0], symb[\"OR\"], gen_wff()]\n",
        "def o_ib(prem):  # Disjunction Introduction B: From A, derive B ∨ A\n",
        "    return [gen_wff(), symb[\"OR\"], prem[0]]\n",
        "# Additional rule for Classical Propositional Logic (CPL)\n",
        "def d_ne(prem):  # Double Negation Elimination: From ¬¬A, derive A\n",
        "    return prem[0][1]\n",
        "\n",
        "# List of rules for IPL and CPL\n",
        "ipl_rules = [fa_e, no_e, n_ia, an_i, a_ea, a_eb, t_ea, t_eb, th_i, o_ia, o_ib]\n",
        "cpl_rules = ipl_rules + [d_ne]\n",
        "\n",
        "# Helper functions\n",
        "def is_negation(f):\n",
        "    return isinstance(f, list) and len(f) == 2 and f[0] == symb[\"NO\"]\n",
        "def is_implication(f):\n",
        "    return isinstance(f, list) and len(f) == 3 and f[1] == symb[\"TH\"]\n",
        "def is_conjunction(f):\n",
        "    return isinstance(f, list) and len(f) == 3 and f[1] == symb[\"AN\"]\n",
        "def is_double_negation(f):\n",
        "    return is_negation(f) and is_negation(f[1])\n",
        "\n",
        "# Function to check applicability of a rule to premises\n",
        "def check(rule, prem):\n",
        "    if len(prem) == 1:\n",
        "        if rule == fa_e:  # Falsum Elimination\n",
        "            if prem[0] == symb[\"FA\"]:\n",
        "                return True\n",
        "        if rule in [a_ea, a_eb]:  # Conjunction Elimination\n",
        "            if is_conjunction(prem[0]):\n",
        "                return True\n",
        "        if rule in [o_ia, o_ib]:  # Disjunction Introduction\n",
        "            return True\n",
        "        if rule == d_ne:  # Double Negation Elimination\n",
        "            if is_double_negation(prem[0]):\n",
        "                return True\n",
        "    elif len(prem) == 2:\n",
        "        if rule == n_ia:  # Negation Introduction\n",
        "            # From A ⊢ ⊥ infer ¬A. Here we assume prem[1] is ⊥\n",
        "            if prem[1] == symb[\"FA\"]:\n",
        "                return True\n",
        "        elif rule == no_e:  # Negation Elimination\n",
        "            if (is_negation(prem[0]) and prem[0][1] == prem[1]) or (is_negation(prem[1]) and prem[1][1] == prem[0]):\n",
        "                return True\n",
        "        elif rule == an_i:  # Conjunction Introduction\n",
        "            return True\n",
        "        elif rule == t_ea:  # Implication Elimination A\n",
        "            if is_implication(prem[1]) and prem[1][0] == prem[0]:\n",
        "                return True\n",
        "        elif rule == t_eb:  # Implication Elimination B\n",
        "            if is_implication(prem[0]) and prem[0][0] == prem[1]:\n",
        "                return True\n",
        "        elif rule == th_i:  # Implication Introduction\n",
        "            # From assumption prem[0] to derive prem[1], infer prem[0] → prem[1]\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Function to get applicable rules for given premises\n",
        "def get_applicable_rules(premises, rules):\n",
        "    applicable_rules = []\n",
        "    for rule in rules:\n",
        "        if check(rule, premises):\n",
        "            applicable_rules.append(rule)\n",
        "    return applicable_rules\n",
        "\n",
        "# Function to generate a derivation\n",
        "def generate_derivation(rules, max_steps=2):\n",
        "    formulas = []\n",
        "    derivation_steps = []\n",
        "    # Start with initial formulas (premises)\n",
        "    num_premises = randint(1, 2)\n",
        "    for _ in range(num_premises):\n",
        "        formula = gen_wff()\n",
        "        formulas.append(formula)\n",
        "        derivation_steps.append({'premises': [], 'conclusion': formula, 'rule': 'Premise'})\n",
        "\n",
        "    num_steps = randint(1, max_steps)\n",
        "    for _ in range(num_steps):\n",
        "        # Choose 1 or 2 formulas from previous formulas\n",
        "        num_premises = choice([1, 2])\n",
        "        if len(formulas) < num_premises:\n",
        "            num_premises = len(formulas)\n",
        "        selected_premises = sample(formulas, num_premises)\n",
        "\n",
        "        # Get applicable rules\n",
        "        applicable_rules = get_applicable_rules(selected_premises, rules)\n",
        "        if applicable_rules:\n",
        "            rule = choice(applicable_rules)\n",
        "            # Apply the rule\n",
        "            new_formula = rule(selected_premises)\n",
        "            formulas.append(new_formula)\n",
        "            derivation_steps.append({'premises': selected_premises, 'conclusion': new_formula, 'rule': rule.__name__})\n",
        "        else:\n",
        "            # No applicable rules, stop derivation\n",
        "            break\n",
        "    premises = [step['conclusion'] for step in derivation_steps if step['rule'] == 'Premise']\n",
        "    conclusion = derivation_steps[-1]['conclusion']\n",
        "    return premises, conclusion, derivation_steps\n",
        "\n",
        "# Function to flatten formulas into tokens\n",
        "def flatten_formula(f):\n",
        "    if isinstance(f, int):\n",
        "        return [f]\n",
        "    elif isinstance(f, list):\n",
        "        tokens = []\n",
        "        if len(f) == 2 and f[0] == symb[\"NO\"]:  # Negation\n",
        "            tokens.append(symb[\"NO\"])\n",
        "            tokens.extend(flatten_formula(f[1]))\n",
        "        elif len(f) == 3:\n",
        "            tokens.append(symb[\"LB\"])\n",
        "            tokens.extend(flatten_formula(f[0]))\n",
        "            tokens.append(f[1])  # Operator\n",
        "            tokens.extend(flatten_formula(f[2]))\n",
        "            tokens.append(symb[\"RB\"])\n",
        "        else:\n",
        "            for subf in f:\n",
        "                tokens.extend(flatten_formula(subf))\n",
        "        return tokens\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "# Prepare data for the transformer model\n",
        "def prepare_data(num_samples=1000):\n",
        "    data = []\n",
        "    for _ in range(num_samples):\n",
        "        premises, conclusion, derivation_steps = generate_derivation(ipl_rules)\n",
        "        if derivation_steps:\n",
        "            input_tokens = []\n",
        "            for premise in premises:\n",
        "                input_tokens.extend(flatten_formula(premise))\n",
        "#                input_tokens.append(symb[\"DE\"])  # Separator between premises\n",
        "            input_tokens.append(symb[\"DE\"])  # Separator before conclusion\n",
        "            input_tokens.extend(flatten_formula(conclusion))\n",
        "            input_tokens.append(symb[\"EOS\"])  # End of sequence token\n",
        "\n",
        "            target_tokens = []\n",
        "            for step in derivation_steps:\n",
        "                target_tokens.extend(flatten_formula(step['conclusion']))\n",
        "                #target_tokens.append(symb[\"DE\"])  # Separator between derivation steps\n",
        "            target_tokens.append(symb[\"EOS\"])  # End of sequence token\n",
        "\n",
        "            data.append((input_tokens, target_tokens))\n",
        "\n",
        "    # Find maximum lengths\n",
        "    max_input_len = max(len(pair[0]) for pair in data)\n",
        "    max_target_len = max(len(pair[1]) for pair in data)\n",
        "\n",
        "    # Pad sequences to the same length and create tensors\n",
        "    padded_inputs = []\n",
        "    padded_targets = []\n",
        "    for input_seq, target_seq in data:\n",
        "        input_seq = input_seq + [symb[\"PAD\"]] * (max_input_len - len(input_seq))\n",
        "        target_seq = [symb[\"SOS\"]] + target_seq + [symb[\"PAD\"]] * (max_target_len - len(target_seq))\n",
        "        padded_inputs.append(input_seq)\n",
        "        padded_targets.append(target_seq)\n",
        "\n",
        "    return padded_inputs, padded_targets, max_input_len, max_target_len + 1  # +1 for SOS\n",
        "\n",
        "# Transformer Model Components\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, num_heads, hidden_dim, num_layers, dropout, max_seq_len=500):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_len, emb_dim))\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "    def forward(self, src, src_mask):\n",
        "        embedded = self.embedding(src) + self.positional_encoding[:, :src.size(1), :].to(src.device)\n",
        "        embedded = embedded.permute(1, 0, 2)  # [seq_len, batch_size, emb_dim]\n",
        "        output = self.transformer_encoder(embedded, src_key_padding_mask=src_mask)\n",
        "        return output\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, num_heads, hidden_dim, num_layers, dropout, max_seq_len=500):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_len, emb_dim))\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
        "    def forward(self, tgt, memory, tgt_mask, tgt_key_padding_mask):\n",
        "        embedded = self.embedding(tgt) + self.positional_encoding[:, :tgt.size(1), :].to(tgt.device)\n",
        "        embedded = embedded.permute(1, 0, 2)  # [seq_len, batch_size, emb_dim]\n",
        "        output = self.transformer_decoder(embedded, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
        "        output = self.fc_out(output)\n",
        "        return output.permute(1, 0, 2)  # [batch_size, seq_len, output_dim]\n",
        "\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask, src_key_padding_mask, tgt_key_padding_mask):\n",
        "        memory = self.encoder(src, src_key_padding_mask)\n",
        "        output = self.decoder(tgt, memory, tgt_mask, tgt_key_padding_mask)\n",
        "        return output\n",
        "\n",
        "# Function to create masks for transformer\n",
        "def create_src_key_padding_mask(seq, pad_idx):\n",
        "    return (seq == pad_idx)\n",
        "\n",
        "def create_tgt_masks(tgt_seq, pad_idx):\n",
        "    tgt_len = tgt_seq.size(1)\n",
        "    # Corrected usage of size for mask creation\n",
        "    tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_len).to(tgt_seq.device)\n",
        "    tgt_key_padding_mask = (tgt_seq == pad_idx)\n",
        "    return tgt_mask, tgt_key_padding_mask\n",
        "\n",
        "def train_model(model, data_loader, num_epochs=10, learning_rate=1e-4):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=symb[\"PAD\"])\n",
        "    model.train()\n",
        "    device = model.device\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, batch in enumerate(data_loader):\n",
        "            src = batch[0].to(device)\n",
        "            tgt = batch[1].to(device)\n",
        "\n",
        "            src_key_padding_mask = create_src_key_padding_mask(src, symb[\"PAD\"])\n",
        "            tgt_input = tgt[:, :-1]  # Remove last token for input\n",
        "            tgt_output = tgt[:, 1:]  # Remove first token for output\n",
        "\n",
        "            tgt_mask, tgt_key_padding_mask = create_tgt_masks(tgt_input, symb[\"PAD\"])\n",
        "            optimizer.zero_grad()\n",
        "            # Perform the model forward pass\n",
        "            output = model(src, tgt_input, None, tgt_mask, src_key_padding_mask, tgt_key_padding_mask)\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            # Reshape output to compare with target output\n",
        "            output_flat = output.reshape(-1, output_dim)\n",
        "            tgt_output_flat = tgt_output.contiguous().view(-1)\n",
        "\n",
        "            loss = criterion(output_flat, tgt_output_flat)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Print 5 model outputs with logical symbols after each epoch\n",
        "            # Print 5 model outputs and corresponding inputs with logical symbols after each epoch\n",
        "            if batch_idx < 1:  # Change to 1 to print from the first batch\n",
        "                predicted_tokens = output.argmax(dim=-1)\n",
        "                print(\"\\nSample model outputs and corresponding inputs:\")\n",
        "\n",
        "                for i in range(min(5, predicted_tokens.size(0))):  # Print up to 5 sequences\n",
        "                    # Decode model outputs\n",
        "                    output_tokens = predicted_tokens[i].cpu().numpy().tolist()\n",
        "                    output_symbols = [symb_reverse.get(token, \"[UNK]\") for token in output_tokens]\n",
        "\n",
        "                    # Decode corresponding inputs\n",
        "                    input_tokens = src[i].cpu().numpy().tolist()\n",
        "                    input_symbols = [symb_reverse.get(token, \"[UNK]\") for token in input_tokens]\n",
        "\n",
        "                    print(f\"Input {i}: {' '.join(input_symbols)}\")\n",
        "                    print(f\"Output {i}: {' '.join(output_symbols)}\\n\")\n",
        "\n",
        "\n",
        "        average_loss = total_loss / len(data_loader)\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss:.4f}')\n",
        "\n",
        "def reconstruct_formula(tokens):\n",
        "    def helper(index):\n",
        "        if index >= len(tokens):\n",
        "            return None, index\n",
        "        tok = tokens[index]\n",
        "        if tok in symb_reverse:\n",
        "            if tok == symb[\"LB\"]:\n",
        "                left_formula, next_index = helper(index + 1)\n",
        "                if next_index >= len(tokens):  # Check if we run out of tokens\n",
        "                    return None, next_index\n",
        "                op = tokens[next_index]\n",
        "                right_formula, next_index = helper(next_index + 1)\n",
        "                if next_index >= len(tokens) or tokens[next_index] != symb[\"RB\"]:  # Extra check for RB\n",
        "                    return None, next_index  # Instead of assertion, return None\n",
        "                return [left_formula, op, right_formula], next_index + 1\n",
        "            elif tok == symb[\"NO\"]:\n",
        "                formula, next_index = helper(index + 1)\n",
        "                return [symb[\"NO\"], formula], next_index\n",
        "            elif tok == symb[\"RB\"] or tok == symb[\"DE\"] or tok == symb[\"EOS\"]:\n",
        "                return None, index\n",
        "            else:\n",
        "                return tok, index + 1\n",
        "        else:\n",
        "            return None, index + 1\n",
        "\n",
        "    formula, _ = helper(0)\n",
        "    return formula\n",
        "\n",
        "def collate_batch(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "    src_batch = torch.stack([torch.tensor(s, dtype=torch.long) for s in src_batch])\n",
        "    tgt_batch = torch.stack([torch.tensor(t, dtype=torch.long) for t in tgt_batch])\n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "def main():\n",
        "    # Prepare data\n",
        "    total_samples = 50000\n",
        "    batch_size = 32\n",
        "    data_inputs, data_targets, max_input_len, max_target_len = prepare_data(num_samples=total_samples)\n",
        "    vocab_size = t_nu + 11  # Number of symbols including PAD, SOS, EOS\n",
        "\n",
        "    # Model parameters\n",
        "    input_dim = vocab_size\n",
        "    output_dim = vocab_size\n",
        "    emb_dim = 256\n",
        "    num_heads = 8\n",
        "    hidden_dim = 512\n",
        "    num_layers = 3\n",
        "    dropout = 0.1\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Initialize the model\n",
        "    encoder = TransformerEncoder(input_dim, emb_dim, num_heads, hidden_dim, num_layers, dropout, max_seq_len=max_input_len)\n",
        "    decoder = TransformerDecoder(output_dim, emb_dim, num_heads, hidden_dim, num_layers, dropout, max_seq_len=max_target_len)\n",
        "    model = Seq2SeqTransformer(encoder, decoder, device).to(device)\n",
        "\n",
        "    # Construct data loader\n",
        "    data = list(zip(data_inputs, data_targets))\n",
        "    data_loader = DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "    # Train the model\n",
        "    model = train_model(model, data_loader, num_epochs=10, learning_rate=1e-4)"
      ],
      "metadata": {
        "id": "ksPIeTVSS9MR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxOrepbhTTC3",
        "outputId": "cc685aeb-d398-432e-f03d-617b39ae498e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample model outputs and corresponding inputs:\n",
            "Input 0: ( q ∧ p ) ⊢ ( p ∨ ( ( q ∧ p ) ∨ r ) ) <EOS>                                                                                                 \n",
            "Output 0: → ¬ ⊥ ∧ ∧ → ¬ ¬ <EOS> ¬ ∧ → → → ∧ ¬ ∧ → ¬ ¬ ⊥ ¬ ∧ → → → → ∧ ⊥ r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r ∧ r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r → r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r\n",
            "\n",
            "Input 1: ¬ q ⊢ ( ( ¬ r ∨ p ) ∨ ¬ q ) <EOS>                                                                                                      \n",
            "Output 1: ∧ r r r r r → r r → r r r ⊢ ⊥ r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r\n",
            "\n",
            "Input 2: t ⊢ ( t ∨ p ) <EOS>                                                                                                             \n",
            "Output 2: ∧ r ¬  r ∧ → → r r r r r r r r r r r r r r ¬ r r r r r r r r r r r r r ∧ r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r <SOS> r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r ∧ r r r r r r r r r r r r r r r r r\n",
            "\n",
            "Input 3: r s ⊢ ( r ∨ r ) <EOS>                                                                                                            \n",
            "Output 3: ∧ r <SOS> r → r → → ⊥ r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r\n",
            "\n",
            "Input 4: ( ( ( t ∧ q ) ∧ r ) → ( ( t ∨ s ) ∧ q ) ) ( ( q ∧ r ) ∧ p ) ⊢ ( ( ( ( t ∧ q ) ∧ r ) → ( ( t ∨ s ) ∧ q ) ) ∧ ( ( q ∧ r ) ∧ p ) ) <EOS>                                                    \n",
            "Output 4: → → → → → → <EOS> → ∧ → → ∧ → → → → <SOS> → → ⊥ → → ¬ → ⊥ → → → → ∧ → → → → → ) → ⊥ → → → → ∧ → → → → ⊢ → ⊢ ⊥ → → → ¬ → ⊥ → → → ∧ ∧ → → → → → → → → ⊥ → → → → → → → → → ⊢ → → ⊥ → → → → → ⊥ → → → → ∧ → → ⊥ r → r r → ) → → ) r ) ) → r ) → → → ) r r r → → r → r → r ) r r r ) ) ) → r → r ) → ) r r ) r ) → ) r r → → → → ) → r ) ) → r\n",
            "\n",
            "Epoch 1/10, Loss: 0.9024\n",
            "\n",
            "Sample model outputs and corresponding inputs:\n",
            "Input 0: ( r ∧ q ) ⊢ ( ( r ∧ q ) → ( ( r ∧ q ) ∨ p ) ) <EOS>                                                                                             \n",
            "Output 0: ( r ∧ ( ) ( ( ( ∧ ( ) ∨ ( ) ( ( ( ∧ q ) → ( r r ∧ q ) ∨ q ) ) <EOS> ( ( ( ( ( ( ( ( ( ( ) ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( r ( ( ( ( r ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( r ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ) ( ( ( ( ( ( (\n",
            "\n",
            "Input 1: q r ⊢ ( q ∧ r ) <EOS>                                                                                                            \n",
            "Output 1: r r ( q ∧ r ) <EOS> r ( ( r ( ( ( ( ( ( ( r ( ( ( r ( ( ( ( r ( ( q ( ( r ( ( ( r r r r r r ( ( r ( ( ( r ( ( r r r ( ( ( ( ( ( ( ( r ( r ( r r r ( r ( r ( ( r r ( r r ( ( ( ( ( ( r r ( ( r ( ( r r ( r ( ( r ( ( ( ( ( ( r ( ( ( r r r ( ( ( ( ( r r r ( ( ( r r ( ( ( r r r r r r ( ( ( r r ( r ( ( r ( ( ( r ( ( ( ( ( r r ( ( r\n",
            "\n",
            "Input 2: t ⊢ ( t ∨ r ) <EOS>                                                                                                             \n",
            "Output 2: t ( r ∨ r ) <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (\n",
            "\n",
            "Input 3: ( ( q → p ) ∧ s ) ⊢ ( ( ( q → r ) ∨ ( ( q → p ) ∧ s ) ) ∨ s ) <EOS>                                                                                     \n",
            "Output 3: ( ( s → s ) ∧ s ) ( ( ( → ( ) ∨ ( ( ( → s ) → s ) ) ( ( ( q → s ) ∨ s ( q → s ) ∨ s ) ) ∨ s ) <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ) ( ( ( ( ( ( ( ) ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (\n",
            "\n",
            "Input 4: q ( q ∧ ( q → ( r ∧ t ) ) ) ⊢ ( q → ( q ∧ ( q → ( r ∧ t ) ) ) ) <EOS>                                                                                    \n",
            "Output 4: ( ( ( ∧ ( ( → ( r ∧ t ) ) ) ( q → ( q → ( r → ( r → t ) ) ) ) <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (\n",
            "\n",
            "Epoch 2/10, Loss: 0.7007\n",
            "\n",
            "Sample model outputs and corresponding inputs:\n",
            "Input 0: ( t ∨ p ) q ⊢ ( ¬ t ∨ ( t ∨ p ) ) <EOS>                                                                                                   \n",
            "Output 0: t t ∨ p ) t ( t t ∨ p t ∨ p ) ) <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (\n",
            "\n",
            "Input 1: q ⊢ ( q ∨ ¬ s ) <EOS>                                                                                                            \n",
            "Output 1: q ( ¬ ∨ ¬ s ) <EOS> ( <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( <EOS> <EOS> ( ( ( ( ( ( ( <EOS> ( ( ( ( ( ( ( ( <EOS> ( <EOS> ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (\n",
            "\n",
            "Input 2: ( ( ( q ∨ s ) ∨ p ) → t ) ⊢ ( r ∨ ( ( ( q ∨ s ) ∨ p ) → t ) ) <EOS>                                                                                     \n",
            "Output 2: ( ( ( p ∨ p ) ∨ ( ) → t ) ( ( ( ( q ∨ r ) → t ) → r ) ∨ ( ( ) <EOS> ( ∨ ( ( ( q → s ) → t ) → t ) ) <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ) ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (\n",
            "\n",
            "Input 3: ( ( r → ( t ∧ q ) ) ∧ p ) ⊢ ( ( ( ( r → ( t ∧ q ) ) ∧ p ) ∨ ( p ∨ ¬ r ) ) ∧ ( ( r → ( t ∧ q ) ) ∧ p ) ) <EOS>                                                                \n",
            "Output 3: ( ( r ∧ p p ∧ p ) ) ∧ p ) ( ( ( ( ∧ ( ( ∧ r ) ) ∧ ¬ ) ∨ ( ( ∧ ¬ ( ) ) ( ( ( ( ( ∧ ( p ) q ) ) ∧ p ) ) ( ( → ( r ) ) ) ( ( ( ∧ ( t ) q ) ) ) ( ) ) <EOS> ( ( ( ( ) ( ) ) ) ) ( ( ) ) ) ) ( ) ) ) ) ) ) ) ) ) ) ) ) ( ( ( ) ) ( ( ) ( ( <EOS> ) ) ( ) ) ( ( ) ) ) ) ) ) ( ( ) ) ) ( ) ) ) ) ) ) ( ) ) ) ) ) ( ( ) ( ) ) ) ( ) ) ( ) ) ) ) )\n",
            "\n",
            "Input 4: s s ⊢ ( s → ( s → s ) ) <EOS>                                                                                                        \n",
            "Output 4: s s ( s → s ) ( s → s s → s ) ) <EOS> s <EOS> <EOS> <EOS> ( ( ( s ( <EOS> <EOS> ( <EOS> ( ( ( ( <EOS> <EOS> s ( <EOS> <EOS> <EOS> ( <EOS> <EOS> ( <EOS> ( <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> s <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> ( <EOS> <EOS> ( ( ( <EOS> <EOS> <EOS> ( <EOS> <EOS> s ( ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( ( <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> s <EOS> <EOS> ( <EOS> ( ( <EOS> <EOS> <EOS> <EOS> <EOS> s ( ( <EOS> <EOS> ( <EOS> <EOS> ( <EOS> ( ( <EOS> ( <EOS> ( <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> s <EOS> ( <EOS> ( s ( <EOS> <EOS> <EOS> <EOS> ( s s <EOS> ( <EOS> ( <EOS> s <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> ( ( <EOS> (\n",
            "\n",
            "Epoch 3/10, Loss: 0.5833\n",
            "\n",
            "Sample model outputs and corresponding inputs:\n",
            "Input 0: ( s → r ) ⊢ ( ( ( s → r ) ∨ ( ( ( p → r ) → ( r ∧ s ) ) ∧ t ) ) → ( s → r ) ) <EOS>                                                                             \n",
            "Output 0: ( s → r ) ( ( s → r ) → ( ( s s → r ) ∧ s s ∧ s ) ) → t ) ) ( ( ( ( → ( ) → ( ( p r → r ) ) r s ) r ) ) ) ( ) ) ) s r → r → ) <EOS> ( ) ( ( ( ( ( ( ) ( ) ) ( ) ) ) ( ( ( ( ( ( ) ( ) ( ) ( ) ( ( ( ( ) ) ) ( ) ) ) ) ( ) ( ( ) ) ( ( ) ) ) ) ( ) ( ( ) ) ) ) ) ) ( ) ) ) ( ) ( ( ) ) ) ) ) ) ( ( ) ) ( ) ) ) ) ) ) ) ( ) ( ) ) ( ) (\n",
            "\n",
            "Input 1: p ⊢ ( p ∨ s ) <EOS>                                                                                                             \n",
            "Output 1: p ( p ∨ s ) <EOS> p ∨ s ) <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ) <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> p <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> p <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> p <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> p <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> p\n",
            "\n",
            "Input 2: t ¬ s ⊢ ( ( ( p ∨ p ) ∨ t ) ∨ ( r ∧ r ) ) <EOS>                                                                                               \n",
            "Output 2: t ¬ s ( ( ( ∨ r ) ∨ r ) ( ( ( r ∨ r ) ∧ r ) ∧ r r ∧ r ) ) <EOS> <EOS> ( ( ( ( <EOS> ( <EOS> <EOS> ( <EOS> <EOS> ( ( <EOS> <EOS> <EOS> ( <EOS> ( <EOS> ( <EOS> <EOS> ( <EOS> ( ( ( <EOS> <EOS> ( ( ( <EOS> <EOS> ( <EOS> <EOS> ( ( <EOS> <EOS> ( ( <EOS> ( ( ( <EOS> ( ( ( <EOS> <EOS> <EOS> <EOS> ( <EOS> ( <EOS> <EOS> <EOS> ( ( <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> ( ( <EOS> ( <EOS> <EOS> ( ( <EOS> ( ( <EOS> ( <EOS> ( ( <EOS> ( ( ( ( <EOS> ( ( ( ( <EOS> ( ( ( <EOS> ( ( ( <EOS> <EOS> ( ( <EOS> <EOS> <EOS> <EOS> <EOS> ( ( ( ( ( ( ( ( ( <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "Input 3: ( s ∨ ( t ∧ t ) ) ⊢ ( ( t ∨ ( s ∨ ( t ∧ t ) ) ) ∨ s ) <EOS>                                                                                         \n",
            "Output 3: ( s ∨ t t ∧ t ) ) ( ( ∨ ( s ∧ t t ∧ t ) ) ) ( ( s ∨ ( s ∧ t t ∧ t ) ) ) ∨ s ) <EOS> ( ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( <EOS> ( ) ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ( ( ) ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ) ( ( ) ( ( (\n",
            "\n",
            "Input 4: s q ⊢ ( q → s ) <EOS>                                                                                                            \n",
            "Output 4: s q ( q → s ) <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> s <EOS> ( ( <EOS> <EOS> ( <EOS> <EOS> <EOS> ( ( q <EOS> ( <EOS> ( ( <EOS> <EOS> <EOS> <EOS> <EOS> ( ( ( <EOS> s <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> ) <EOS> ( ( ) <EOS> <EOS> <EOS> <EOS> <EOS> s <EOS> <EOS> <EOS> <EOS> s <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> s ( <EOS> ( <EOS> <EOS> <EOS> <EOS> s <EOS> <EOS> <EOS> <EOS> s <EOS> <EOS> ( ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( ( <EOS> <EOS> <EOS> <EOS> s <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> ( s <EOS> <EOS> ( ( <EOS> <EOS> <EOS> ( <EOS> ( <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> s <EOS> ( ( <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "Epoch 4/10, Loss: 0.4688\n",
            "\n",
            "Sample model outputs and corresponding inputs:\n",
            "Input 0: p ⊢ ( s ∨ ( p ∨ t ) ) <EOS>                                                                                                         \n",
            "Output 0: p ( s ∨ s ) ( s ∨ ( p ∨ t ) ) <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> ( ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> ( ( ( ( ( ( ( ( <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> ( <EOS> ( <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> ( <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "Input 1: ( t ∧ t ) q ⊢ ( p ∨ ( t ∧ t ) ) <EOS>                                                                                                    \n",
            "Output 1: ( t ∧ q ) q ( ( ∨ ( p ∨ t ) ) ( p ∨ t q ∧ t ) ) <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> t <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> t <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> t t <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> t <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> t <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> t <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "Input 2: ( ( q ∨ q ) ∨ t ) ⊢ ( ( ( q ∨ q ) ∨ t ) ∨ ( s ∨ t ) ) <EOS>                                                                                         \n",
            "Output 2: ( ( q ∨ q ) ∨ t ) ( ( ( q ∨ q ) ∨ t ) ∨ t ( ∨ t ) ) <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (\n",
            "\n",
            "Input 3: ( ¬ t → t ) ⊢ ( ( ( ¬ t → t ) ∨ ( ( s → r ) → ¬ ( s → r ) ) ) → ( ¬ t → t ) ) <EOS>                                                                             \n",
            "Output 3: ( ¬ t → t ) ( ( ¬ t → s ) ∨ ( ¬ ¬ → r ) → ¬ t r → t ) ) ) ( ( ¬ ¬ t → ¬ ) → ( ¬ s → r ) → ¬ t s → t ) ) → → ¬ ¬ t → ¬ ) ) ) ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( <EOS> ( ( ( ( ( ( ( <EOS> <EOS> ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( <EOS> ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (\n",
            "\n",
            "Input 4: s ⊢ ( s ∨ ( t ∧ ( t ∧ ( q → r ) ) ) ) <EOS>                                                                                                 \n",
            "Output 4: s ( s ∨ ( ) ( s ∧ ( t ∧ ( t ∧ ( q → r ) ) ) ) <EOS> ( ( ( ( ( ( ( ( ( ( ( <EOS> <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( <EOS> ( ( ( ( ( ( ( <EOS> ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( ( <EOS> ( (\n",
            "\n",
            "Epoch 5/10, Loss: 0.3871\n",
            "\n",
            "Sample model outputs and corresponding inputs:\n",
            "Input 0: q ⊢ ( ( r ∧ p ) ∨ q ) <EOS>                                                                                                         \n",
            "Output 0: q ( r r ∧ p ) ∨ q ) <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> ( <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( ( ( ( ( <EOS> <EOS> <EOS> ( <EOS> <EOS> ( <EOS> <EOS> ( ( <EOS> ( <EOS> ( <EOS> <EOS> ( ( ( <EOS> <EOS> <EOS> <EOS> ( <EOS> ( <EOS> <EOS> <EOS> <EOS> ( <EOS> ( <EOS> <EOS> <EOS> <EOS> ( ( <EOS> <EOS> ( <EOS> ( <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> ( <EOS> <EOS> ( <EOS> ( ( ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> ( <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> ( <EOS> ( ( <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS>\n",
            "\n",
            "Input 1: p p ⊢ ( p → ( p ∨ p ) ) <EOS>                                                                                                        \n",
            "Output 1: p p ( p ∨ p ) ( p → ( p ∨ p ) ) <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (\n",
            "\n",
            "Input 2: q t ⊢ ( q ∧ t ) <EOS>                                                                                                            \n",
            "Output 2: q t ( q ∧ t ) <EOS> ( <EOS> ( ( t ( <EOS> <EOS> <EOS> ( ( q <EOS> <EOS> <EOS> ( ( <EOS> ( ( ( <EOS> <EOS> ( ( ( <EOS> <EOS> q ( q ( ( <EOS> <EOS> <EOS> <EOS> <EOS> ( ( <EOS> <EOS> ( <EOS> q <EOS> t <EOS> <EOS> <EOS> t <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> t <EOS> q <EOS> <EOS> q <EOS> <EOS> q ( ( <EOS> ( <EOS> <EOS> <EOS> ( ( <EOS> ( <EOS> <EOS> <EOS> <EOS> q <EOS> q ( ( ( ( ( <EOS> t ( <EOS> q <EOS> <EOS> <EOS> <EOS> ( <EOS> ( t <EOS> <EOS> <EOS> ( <EOS> q t q ( t ( q <EOS> <EOS> <EOS> ( <EOS> ( <EOS> <EOS> <EOS> ( ( ( <EOS> <EOS> <EOS> ( ( <EOS> ( ( ( ( t <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> t ( <EOS> ( <EOS> (\n",
            "\n",
            "Input 3: ( q ∨ ¬ p ) ⊢ ( ( q ∨ ¬ p ) ∨ s ) <EOS>                                                                                                   \n",
            "Output 3: ( q ∨ ¬ p ) ( q q ∨ ¬ p ) ∨ s s ∨ s ( ( ∨ s ) ∨ ( ) ) ) ( q q ∨ ¬ p ) ∨ s ) <EOS> <EOS> ( <EOS> ( ( <EOS> ( <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ¬ <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> ( <EOS> ( <EOS> ¬ ( ( <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( ¬ <EOS> <EOS> <EOS> ) <EOS> ( <EOS> <EOS> <EOS> <EOS> ( <EOS> ( <EOS> ( ( <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> ( ( ( <EOS> <EOS> <EOS> <EOS> ( ( <EOS> ( ( <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> ) ( <EOS> ( ) <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> ( <EOS> <EOS> ( <EOS>\n",
            "\n",
            "Input 4: r ⊢ ( ( ( t → t ) ∧ p ) ∨ r ) <EOS>                                                                                                     \n",
            "Output 4: r ( ( t t → p ) ∧ p ) ∨ r ) <EOS> ( ( ( ( ( ( ( <EOS> ( ( <EOS> ( ( ( ( ( ( ( ( ( ( <EOS> ( ( ( <EOS> ( ( <EOS> ( ( ( ( ( ( ( <EOS> ( <EOS> <EOS> ( ( ( <EOS> <EOS> ( ( <EOS> ( ( ( ( ( ( ( ( ( <EOS> ( ( ( <EOS> ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( ( <EOS> <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( <EOS> ( ( ( ( <EOS> ( ( <EOS> ( <EOS> ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( (\n",
            "\n",
            "Epoch 6/10, Loss: 0.3273\n",
            "\n",
            "Sample model outputs and corresponding inputs:\n",
            "Input 0: p ⊢ ( t ∨ ( p ∨ ( ( t → ( q ∨ t ) ) ∧ ( s ∨ q ) ) ) ) <EOS>                                                                                         \n",
            "Output 0: p ( p ∨ ( t t ∨ q q ∨ t ) ) ∧ ( q ∨ q ) ) ) <EOS> t ∨ ( p ∨ ( ( t → ( q ) t ) ) ∧ ( s ∨ q ) ) ) ) <EOS> <EOS> ( ( ( ( <EOS> ( ( <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> ( ( ( <EOS> <EOS> ( <EOS> ( ( ( <EOS> <EOS> <EOS> ( ( ( <EOS> <EOS> <EOS> <EOS> <EOS> ( ( ( ( ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( ( ( ( <EOS> <EOS> ( <EOS> ( <EOS> ( <EOS> ( <EOS> ( ( ( ( ( ( <EOS> ( <EOS> <EOS> ( ( ( ( <EOS> <EOS> ( <EOS> <EOS> ( ( <EOS> ( ( <EOS> ( ( <EOS> <EOS> <EOS> <EOS> ( ( <EOS> ( <EOS> ( ( ( ( ( ( <EOS> ( ( ( <EOS> ( <EOS> ( <EOS>\n",
            "\n",
            "Input 1: ( q ∧ p ) ⊢ ( ( q ∧ p ) → ( s ∨ ( q ∧ p ) ) ) <EOS>                                                                                             \n",
            "Output 1: ( q ∧ p ) ( ( ∨ ( q ∧ p ) ) ( ( q ∧ p ) → ( s ∨ ( q ∧ p ) ) ) <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (\n",
            "\n",
            "Input 2: r ⊢ ( ( s ∨ r ) ∧ r ) <EOS>                                                                                                         \n",
            "Output 2: r ( s ∨ r ) ( ( s ∨ r ) ∧ r ) <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "Input 3: ( p ∨ ( ( q → t ) ∧ ( p ∨ r ) ) ) ⊢ ( ( p ∨ ( ( q → t ) ∧ ( p ∨ r ) ) ) ∨ ¬ s ) <EOS>                                                                            \n",
            "Output 3: ( p ∨ ( ( q → t ) ∧ ( p ∨ r ) ) ) ( ( p ∨ ( q q → t ) ∧ ( p ∨ r ) ) ) ∨ ( s ) <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (\n",
            "\n",
            "Input 4: t ⊢ ( t ∨ q ) <EOS>                                                                                                             \n",
            "Output 4: t ( t ∨ q ) <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> ( ( ( <EOS> <EOS> <EOS> <EOS> <EOS> ( ( ( <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> ( <EOS> ( <EOS> ( ( <EOS> ( ( <EOS> ( ( <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> ( <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> ( ( <EOS> <EOS> ( <EOS> ( ( <EOS> <EOS> <EOS> ( <EOS> <EOS> ( ( <EOS> ( <EOS> ( ( <EOS> <EOS> <EOS> ( ( <EOS> ( <EOS> <EOS> ( <EOS> ( <EOS> <EOS> ( <EOS> <EOS> <EOS> ( ( <EOS> ( ( <EOS> ( ( ( <EOS> ( ( <EOS> <EOS> ( ( <EOS> ( ( ( <EOS> ( <EOS> <EOS> ( ( ( ( ( <EOS> ( ( <EOS> ( <EOS> <EOS> ( <EOS> <EOS> ( (\n",
            "\n",
            "Epoch 7/10, Loss: 0.2763\n",
            "\n",
            "Sample model outputs and corresponding inputs:\n",
            "Input 0: ( ( r ∨ r ) → q ) ⊢ ( ( ( ( r ∨ r ) → q ) ∨ p ) → ( ( r ∨ r ) → q ) ) <EOS>                                                                                 \n",
            "Output 0: ( ( r ∨ r ) → q ) ( ( ( r ∨ r ) → q ) ∨ ( ) ( ( ( ( r ∨ r ) → q ) ∨ p ) → ( ( r ∨ r ) → q ) ) <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (\n",
            "\n",
            "Input 1: ( s → t ) ⊢ ( ( s → t ) → ( ¬ r ∨ ( s → t ) ) ) <EOS>                                                                                            \n",
            "Output 1: ( s → t ) ( ( ( ∨ ( s → t ) ) ( ( s → t ) → ( ¬ r ∨ ( s → t ) ) ) <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (\n",
            "\n",
            "Input 2: ( r ∧ ¬ p ) s ⊢ r <EOS>                                                                                                           \n",
            "Output 2: ( r ∧ ¬ p ) s r r ∧ ( r ∧ ¬ p ) ) r <EOS> r r <EOS> r <EOS> r <EOS> r <EOS> <EOS> r r <EOS> <EOS> r p <EOS> r r <EOS> r r r <EOS> r <EOS> <EOS> <EOS> <EOS> r r <EOS> <EOS> r r r <EOS> r r <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> r <EOS> <EOS> r r <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> r <EOS> <EOS> <EOS> r r <EOS> r <EOS> r <EOS> r <EOS> <EOS> <EOS> <EOS> r r <EOS> r <EOS> r <EOS> r <EOS> <EOS> r r r <EOS> <EOS> <EOS> r ( ( r <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> r p r r p r <EOS> <EOS> ) <EOS> <EOS> r <EOS> <EOS> r r r <EOS> r ) r r <EOS> p r <EOS> r r <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> r <EOS> <EOS> r r\n",
            "\n",
            "Input 3: q s ⊢ ( t ∨ ( s ∨ p ) ) <EOS>                                                                                                        \n",
            "Output 3: q s ( s ∨ ( ) ( t ∨ ( s ∨ p ) ) <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( <EOS> ( ( ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( <EOS> ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ( <EOS>\n",
            "\n",
            "Input 4: ( s ∨ ( ¬ p ∧ r ) ) t ⊢ ( t → ( s ∨ ( ¬ p ∧ r ) ) ) <EOS>                                                                                          \n",
            "Output 4: ( s ∨ ¬ ¬ p ∧ r ) ) t ( ( → ( s ∨ ( ¬ p ∧ r ) ) ) <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (\n",
            "\n",
            "Epoch 8/10, Loss: 0.2324\n",
            "\n",
            "Sample model outputs and corresponding inputs:\n",
            "Input 0: t ⊢ ( t ∨ ( ( q → ( p → t ) ) ∨ ( r ∧ s ) ) ) <EOS>                                                                                             \n",
            "Output 0: t ( t ∨ ( ) ( t ∨ ( ( q → ( p → t ) ) ∨ ( r ∧ s ) ) ) <EOS> <EOS> ( ( ( ( ( ( ( <EOS> <EOS> ( <EOS> <EOS> ( ( ( ( ( <EOS> ( <EOS> <EOS> ( ( ( ( ( ( ( <EOS> <EOS> ( <EOS> ( <EOS> ( ( ( ( <EOS> <EOS> ( <EOS> <EOS> ( ( ( <EOS> ( <EOS> <EOS> <EOS> <EOS> ( <EOS> ( <EOS> ( ( ( ( ( <EOS> ( ( <EOS> <EOS> <EOS> ( <EOS> ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( ( <EOS> ( ( <EOS> <EOS> ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( <EOS> ( <EOS> ( ( ( ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( <EOS> ( (\n",
            "\n",
            "Input 1: r ⊢ ( r ∨ ( q → s ) ) <EOS>                                                                                                         \n",
            "Output 1: r ( r ∨ ( q → s ) ) <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (\n",
            "\n",
            "Input 2: ( ¬ q ∧ r ) r ⊢ ( ( ( ¬ q ∧ r ) → r ) → r ) <EOS>                                                                                              \n",
            "Output 2: ( ¬ q ∧ r ) r ( ( ¬ q ∧ r ) → r ) ( ( ( ¬ q ∧ r ) → r ) → r ) <EOS> <EOS> ( ( ( ( ( <EOS> ( ( r r <EOS> ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( <EOS> ( ( ( ( ( <EOS> ( ( ( ( r ( ( r ( ( ( ( ( <EOS> ( <EOS> ( <EOS> ( ( ( ( ( ( ( r r ( <EOS> <EOS> ( <EOS> ( <EOS> ( ( ( ( ( ( ( <EOS> ( ( <EOS> ( ( ( ( ( <EOS> ( ( <EOS> ( ( ( r <EOS> ( ( <EOS> ( ( ( ( <EOS> <EOS> ( ( ( <EOS> <EOS> r ( ( ( ( ( <EOS> ( ( ( ( <EOS> ( ( <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> ( <EOS>\n",
            "\n",
            "Input 3: ¬ ( q ∨ ( q → t ) ) ⊢ ( ¬ ( q ∨ ( q → t ) ) ∨ ( r ∨ ( s ∨ ( t ∧ t ) ) ) ) <EOS>                                                                               \n",
            "Output 3: ¬ ( q ∨ ( q → t ) ) ( ¬ ( q ∨ ( q → t ) ) ∨ ( ( ∨ ( s ∨ t s ∧ t ) ) ) ) <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (\n",
            "\n",
            "Input 4: ( s ∨ ( t ∨ ( q → q ) ) ) p ⊢ ( p ∨ ( s → ( p ∨ ( q → t ) ) ) ) <EOS>                                                                                    \n",
            "Output 4: ( s ∨ ( t ∨ ( q → q ) ) ) p ( p ∨ ( s → ( t ∨ ( q → t ) ) ) ) <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (\n",
            "\n",
            "Epoch 9/10, Loss: 0.2006\n",
            "\n",
            "Sample model outputs and corresponding inputs:\n",
            "Input 0: t ⊢ ( t ∨ p ) <EOS>                                                                                                             \n",
            "Output 0: t ( t ∨ p ) <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (\n",
            "\n",
            "Input 1: r ( ¬ s → r ) ⊢ ( ( ¬ s → r ) ∨ ( ( ( t ∧ q ) → s ) → ¬ ( p ∧ r ) ) ) <EOS>                                                                                 \n",
            "Output 1: r ( ¬ s → r ) ( ( ¬ s → r ) ∨ ( ( t t ∧ s ) → ¬ ) → ¬ ( p ∧ r ) ) ) <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (\n",
            "\n",
            "Input 2: s ⊢ ( s ∨ r ) <EOS>                                                                                                             \n",
            "Output 2: s ( s ∨ r ) <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> ( ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> r <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "Input 3: p ⊢ ( ( t ∨ p ) ∨ r ) <EOS>                                                                                                         \n",
            "Output 3: p ( ( ∨ p ) ( ( t ∨ p ) ∨ r ) <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> ( <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "Input 4: ¬ ( ( p → t ) ∧ ( s ∧ q ) ) ( ( ( p ∨ p ) ∨ r ) ∨ r ) ⊢ ( ( ( ( ( p ∨ p ) ∨ r ) ∨ r ) ∨ q ) → ( ( ( p ∨ p ) ∨ r ) ∨ r ) ) <EOS>                                                       \n",
            "Output 4: ¬ ( ( p → t ) ∧ ( s ∧ q ) ) ( ( ( p ∨ p ) ∨ r ) ∨ r ) ( ( ( ( p ∨ p ) ∨ r ) ∨ r ) ∨ ( ) ( ( ( ( ( p ∨ p ) ∨ r ) ∨ r ) ∨ ( ) ) ( ( ( ( ∨ r ) ∨ r ) ) r ) ) <EOS> ( ( <EOS> ( ( ( <EOS> ( ( ( ( ( ( <EOS> ( ( ( ( <EOS> ( ( ( ( ( ( <EOS> ( ( ( <EOS> <EOS> ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( <EOS> ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( ( ( ( ( ( ( ( ( ( <EOS> ( ( <EOS> (\n",
            "\n",
            "Epoch 10/10, Loss: 0.1764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    premises, conclusion, derivation_steps = generate_derivation(ipl_rules)\n",
        "    if derivation_steps:\n",
        "        input_tokens = []\n",
        "        for premise in premises:\n",
        "            input_tokens.extend(flatten_formula(premise))\n",
        "            input_tokens.append(symb[\"DE\"])  # Separator between premises\n",
        "        input_tokens.append(symb[\"DE\"])  # Separator before conclusion\n",
        "        input_tokens.extend(flatten_formula(conclusion))\n",
        "        input_tokens.append(symb[\"EOS\"])  # End of sequence token\n",
        "\n",
        "        src = torch.tensor([input_tokens], dtype=torch.long).to(device)\n",
        "        src_key_padding_mask = create_src_key_padding_mask(src, symb[\"PAD\"])\n",
        "\n",
        "        max_length = 50  # Maximum length of generated sequence\n",
        "        tgt_tokens = [symb[\"SOS\"]]\n",
        "        for i in range(max_length):\n",
        "            tgt_seq = torch.tensor([tgt_tokens], dtype=torch.long).to(device)\n",
        "            tgt_mask, tgt_key_padding_mask = create_tgt_masks(tgt_seq, symb[\"PAD\"])\n",
        "            output = model(src, tgt_seq, None, tgt_mask, src_key_padding_mask, tgt_key_padding_mask)\n",
        "            next_token = output.argmax(dim=-1)[:, -1].item()\n",
        "            tgt_tokens.append(next_token)\n",
        "            if next_token == symb[\"EOS\"]:\n",
        "                break\n",
        "\n",
        "        predicted_tokens = tgt_tokens[1:]  # Remove SOS token\n",
        "\n",
        "        # Convert tokens to formulas\n",
        "        predicted_formulas = []\n",
        "        current_formula_tokens = []\n",
        "        for tok in predicted_tokens:\n",
        "            if tok == symb[\"DE\"]:\n",
        "                if current_formula_tokens:\n",
        "                    formula = reconstruct_formula(current_formula_tokens)\n",
        "                    predicted_formulas.append(formula)\n",
        "                    current_formula_tokens = []\n",
        "            elif tok == symb[\"EOS\"]:\n",
        "                if current_formula_tokens:\n",
        "                    formula = reconstruct_formula(current_formula_tokens)\n",
        "                    predicted_formulas.append(formula)\n",
        "                break\n",
        "            else:\n",
        "                current_formula_tokens.append(tok)\n",
        "\n",
        "        # Print the premises and conclusion\n",
        "        print(\"Premises and Conclusion:\")\n",
        "        for premise in premises:\n",
        "            print(f\"Premise: {formula_to_string(premise)}\")\n",
        "        print(f\"Conclusion: {formula_to_string(conclusion)}\")\n",
        "\n",
        "        # Print the predicted derivation\n",
        "        print(\"\\nPredicted Derivation Steps:\")\n",
        "        for f in predicted_formulas:\n",
        "            if f:\n",
        "                print(formula_to_string(f))\n",
        "            else:\n",
        "                print(\"Invalid formula\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "elzuFzcjmlZT",
        "outputId": "c170ed1a-01b5-4174-e181-6f7780e4dac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-c465eba23d52>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Testing the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpremises\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconclusion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderivation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_derivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipl_rules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mderivation_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    }
  ]
}